{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from advsecurenet.models.model_factory import ModelFactory\n",
    "from advsecurenet.datasets import DatasetFactory\n",
    "from advsecurenet.dataloader import DataLoaderFactory\n",
    "from advsecurenet.shared.types import DatasetType\n",
    "from advsecurenet.utils.model_utils import train as train_model, test as test_model, save_model\n",
    "from advsecurenet.shared.types.configs.train_config import TrainConfig\n",
    "from advsecurenet.defenses import AdversarialTraining\n",
    "from advsecurenet.attacks.fgsm import FGSM\n",
    "from advsecurenet.attacks.pgd import PGD\n",
    "from advsecurenet.shared.types.configs.defense_configs.adversarial_training_config import AdversarialTrainingConfig\n",
    "import advsecurenet.shared.types.configs.attack_configs as AttackConfigs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load basic MNIST model\n",
    "mnist_model = ModelFactory.get_model(model_variant='CustomMnistModel', num_classes=10, num_input_channels=1)\n",
    "mnist_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load MNIST dataset\n",
    "dataset = DatasetFactory.load_dataset(DatasetType.MNIST)\n",
    "train_data = dataset.load_dataset(train=True)\n",
    "test_data = dataset.load_dataset(train=False)\n",
    "train_loader = DataLoaderFactory.get_dataloader(dataset=train_data, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoaderFactory.get_dataloader(dataset=test_data, batch_size=128, shuffle=False)\n",
    "print(f\"Train dataset size: {len(train_data)}\")\n",
    "print(f\"Test dataset size: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first normal training\n",
    "train_config = TrainConfig(model= mnist_model, train_loader=train_loader, epochs=1, device=\"mps\")\n",
    "train_model(train_config)\n",
    "test_model(mnist_model, test_loader,device=\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to test model robustness against different attacks\n",
    "def test_model_robustness(model, test_loader, attack, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    # Initialize counters\n",
    "    correct = 0\n",
    "    adv_correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for data, target in tqdm(test_loader, desc='Testing'):\n",
    "        # Send data and target to the same device as your model\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # Get the original model's predictions\n",
    "        output = model(data)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        \n",
    "        # Generate adversarial data using the provided attack method\n",
    "        fgsm_data = attack.attack(model=model, x=data, y=target)\n",
    "        \n",
    "        # Get the model's predictions on the adversarial data\n",
    "        adv_output = model(fgsm_data)\n",
    "        adv_pred = adv_output.argmax(dim=1, keepdim=True)\n",
    "        adv_correct += adv_pred.eq(target.view_as(adv_pred)).sum().item()\n",
    "\n",
    "        total += target.size(0)\n",
    "\n",
    "    # Calculate the original accuracy\n",
    "    original_accuracy = correct / total\n",
    "\n",
    "    # Calculate the adversarial accuracy\n",
    "    adversarial_accuracy = adv_correct / total\n",
    "\n",
    "    # Calculate the robustness as the difference in accuracies\n",
    "    robustness = original_accuracy - adversarial_accuracy\n",
    "\n",
    "    print(f'Original Accuracy: {original_accuracy:.2%}')\n",
    "    print(f'Adversarial Accuracy: {adversarial_accuracy:.2%}')\n",
    "    print(f'Robustness (Accuracy Drop): {robustness:.2%}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get FGSM attack\n",
    "fgsm_config = AttackConfigs.FgsmAttackConfig(epsilon=0.5, device=\"mps\")\n",
    "fgsm = FGSM(fgsm_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing base model against FGSM attack \n",
    "test_model_robustness(mnist_model, test_loader, fgsm, device=\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use fgsm to adversarially train the model\n",
    "robust_model = ModelFactory.get_model(model_variant='CustomMnistModel', num_classes=10, num_input_channels=1)\n",
    "adversarial_training_config = AdversarialTrainingConfig(model=robust_model, models=[robust_model], attacks=[fgsm], train_loader=train_loader, epochs=5, device=\"mps\")\n",
    "adversarial_training = AdversarialTraining(adversarial_training_config)\n",
    "adversarial_training.adversarial_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the clean accuracy of the adversarially trained model\n",
    "test_model(robust_model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
